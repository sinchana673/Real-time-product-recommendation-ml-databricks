{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed95047f-977d-4dc3-8ae7-14764b1fb669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## üìí Notebook Summary: ML Training Data Preparation\n",
    "\n",
    "This notebook covers the following steps for preparing ML training and validation datasets for product recommendation:\n",
    "\n",
    "1. **Read Feature Store Data**\n",
    "   - Loaded candidate features for user-product pairs from the feature store.\n",
    "\n",
    "2. **Extract Candidate Time**\n",
    "   - Computed the latest interaction timestamp per user for time-based splitting.\n",
    "\n",
    "3. **Define Training Cutoff**\n",
    "   - Established a global cutoff time (e.g., 30 days before the latest event) to separate past and future data.\n",
    "\n",
    "4. **Generate Labels**\n",
    "   - Identified future purchases after the cutoff as positive labels.\n",
    "   - Joined features with labels, assigning 1 for future purchases and 0 otherwise.\n",
    "\n",
    "5. **Time-Based Train/Validation Split**\n",
    "   - Split data into train and validation sets based on candidate interaction time (e.g., last 7 days for validation).\n",
    "\n",
    "6. **Filter Train Users**\n",
    "   - Removed users from the training set who have no positive labels to ensure meaningful training.\n",
    "\n",
    "7. **Create Final Datasets**\n",
    "   - Dropped unnecessary columns and prepared ranking-ready train and validation DataFrames.\n",
    "\n",
    "8. **Snapshot Data**\n",
    "   - Saved train and validation datasets as Delta tables with unique snapshot IDs and timestamps for reproducibility.\n",
    "\n",
    "9. **Sanity Checks**\n",
    "   - Displayed label distributions for both train and validation sets to verify data balance.\n",
    "\n",
    "---\n",
    "\n",
    "**Outcome:**  \n",
    "You now have reproducible, leakage-free train and validation datasets for ML model development, with all steps documented and data snapshots saved for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0535dcf2-5259-42c9-aa1b-e67838a0b31b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "import uuid\n",
    "\n",
    "spark.conf.set(\"spark.databricks.remoteFiltering.blockSelfJoins\", \"false\")\n",
    "\n",
    "gold = \"kusha_solutions.product_recomendation\"\n",
    "\n",
    "# ============================================================\n",
    "# üîí FIXED TIME CONFIG (MATCHES FAKER + FEATURE ENG)\n",
    "# ============================================================\n",
    "FEATURE_CUTOFF = \"2025-12-01 00:00:00\"\n",
    "VALID_CUTOFF   = \"2025-12-20 00:00:00\"\n",
    "\n",
    "print(\"üìå Feature cutoff :\", FEATURE_CUTOFF)\n",
    "print(\"üìå Validation cutoff :\", VALID_CUTOFF)\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ READ FROZEN FEATURES (LEAKAGE-FREE)\n",
    "# ============================================================\n",
    "features = spark.table(\n",
    "    \"kusha_solutions.product_recomendation.fs_canddiate_features\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ TRAIN LABELS (PURCHASES BETWEEN CUToffs)\n",
    "# ============================================================\n",
    "train_purchases = (\n",
    "    spark.table(f\"{gold}.gold_sales_enriched\")\n",
    "         .filter(F.col(\"EventTime\") >= FEATURE_CUTOFF)\n",
    "         .filter(F.col(\"EventTime\") < VALID_CUTOFF)\n",
    "         .filter(F.lower(F.col(\"InteractionType\")) == \"purchase\")\n",
    "         .select(\"CustomerID\", \"ProductID\")\n",
    "         .distinct()\n",
    "         .withColumn(\"label\", F.lit(1))\n",
    ")\n",
    "\n",
    "train_df = (\n",
    "    features\n",
    "    .join(train_purchases, [\"CustomerID\", \"ProductID\"], \"left\")\n",
    "    .withColumn(\"label\", F.coalesce(\"label\", F.lit(0)))\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ VALIDATION LABELS (PURCHASES AFTER VALID CUTOFF)\n",
    "# ============================================================\n",
    "valid_purchases = (\n",
    "    spark.table(f\"{gold}.gold_sales_enriched\")\n",
    "         .filter(F.col(\"EventTime\") >= VALID_CUTOFF)\n",
    "         .filter(F.lower(F.col(\"InteractionType\")) == \"purchase\")\n",
    "         .select(\"CustomerID\", \"ProductID\")\n",
    "         .distinct()\n",
    "         .withColumn(\"label\", F.lit(1))\n",
    ")\n",
    "\n",
    "valid_df = (\n",
    "    features\n",
    "    .join(valid_purchases, [\"CustomerID\", \"ProductID\"], \"left\")\n",
    "    .withColumn(\"label\", F.coalesce(\"label\", F.lit(0)))\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ SANITY CHECK\n",
    "# ============================================================\n",
    "print(\"üìä TRAIN label distribution\")\n",
    "train_df.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"üìä VALID label distribution\")\n",
    "valid_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# ============================================================\n",
    "# 5Ô∏è‚É£ SNAPSHOT TRAIN & VALID (REPRODUCIBLE)\n",
    "# ============================================================\n",
    "snapshot_id = str(uuid.uuid4())\n",
    "\n",
    "train_snapshot = (\n",
    "    train_df\n",
    "      .withColumn(\"snapshot_id\", lit(snapshot_id))\n",
    "      .withColumn(\"snapshot_ts\", current_timestamp())\n",
    "      .withColumn(\"feature_cutoff\", lit(FEATURE_CUTOFF))\n",
    "      .withColumn(\"valid_cutoff\", lit(VALID_CUTOFF))\n",
    ")\n",
    "\n",
    "train_snapshot.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(f\"{gold}.ml_train_snapshot\")\n",
    "\n",
    "valid_snapshot = (\n",
    "    valid_df\n",
    "      .withColumn(\"snapshot_id\", lit(snapshot_id))\n",
    "      .withColumn(\"snapshot_ts\", current_timestamp())\n",
    "      .withColumn(\"feature_cutoff\", lit(FEATURE_CUTOFF))\n",
    "      .withColumn(\"valid_cutoff\", lit(VALID_CUTOFF))\n",
    ")\n",
    "\n",
    "valid_snapshot.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(f\"{gold}.ml_valid_snapshot\")\n",
    "\n",
    "print(\"‚úÖ Time-based train & validation snapshots saved\")\n",
    "print(\"üìå Snapshot ID:\", snapshot_id)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7_Data_splitting",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
