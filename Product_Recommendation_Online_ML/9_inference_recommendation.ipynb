{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "791d174a-a941-495f-aa1a-802a77a00a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Notebook Summary: Product Recommendation Pipeline\n",
    "\n",
    "**Steps performed in this notebook:**\n",
    "\n",
    "1. **Configuration Setup**\n",
    "   - Defined database and table names, model URI, and parameters for recommendations.\n",
    "\n",
    "2. **Incremental Run Logic**\n",
    "   - Checked for previous runs and determined the start time for incremental inference.\n",
    "\n",
    "3. **Model Loading**\n",
    "   - Loaded ML model for product ranking using MLflow.\n",
    "\n",
    "4. **Data Preparation**\n",
    "   - Loaded sales, customer, and product tables.\n",
    "   - Derived customer age groups.\n",
    "\n",
    "5. **User Segmentation**\n",
    "   - Segmented users into new active, recently active, historical, and cold users.\n",
    "\n",
    "6. **Recommendation Generation**\n",
    "   - For active users: Generated ML + rule-based hybrid recommendations.\n",
    "   - For cold users: Generated context-aware popular product recommendations.\n",
    "\n",
    "7. **Result Union & Snapshot Write**\n",
    "   - Combined recommendations and wrote results to the snapshot table.\n",
    "\n",
    "8. **Preview**\n",
    "   - Displayed the latest recommendations for review.\n",
    "\n",
    "---\n",
    "**Outcome:**  \n",
    "Automated, incremental product recommendations for users, stored and previewed in a unified table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e6ff1b-6e14-4eda-b120-1221b14db82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pandas as pd\n",
    "import mlflow.pyfunc\n",
    "from datetime import datetime\n",
    "\n",
    "spark.conf.set(\"spark.databricks.remoteFiltering.blockSelfJoins\", \"false\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "GOLD_DB   = \"kusha_solutions.product_recomendation\"\n",
    "SILVER_DB = \"kusha_solutions.product_recomendation\"\n",
    "\n",
    "RECO_TABLE = f\"{GOLD_DB}.user_recommendation_snapshots_live\"\n",
    "\n",
    "MODEL_URI     = \"models:/kusha_solutions.default.product_recommendation_ranker@prod\"\n",
    "MODEL_VERSION = \"v2\"\n",
    "\n",
    "TOP_K = 10\n",
    "REFRESH_DAYS = 7\n",
    "\n",
    "# üîí STRICT INFERENCE START (AFTER JAN 5)\n",
    "INFERENCE_START_TS = \"2026-01-06 00:00:00\"\n",
    "\n",
    "# ============================================================\n",
    "# 0Ô∏è‚É£ DETERMINE INCREMENTAL START TIME (FIXED)\n",
    "# ============================================================\n",
    "\n",
    "def table_exists(name):\n",
    "    try:\n",
    "        spark.table(name)\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "if table_exists(RECO_TABLE):\n",
    "    last_run_ts = (\n",
    "        spark.table(RECO_TABLE)\n",
    "             .select(F.max(\"run_ts\").alias(\"ts\"))\n",
    "             .collect()[0][\"ts\"]\n",
    "    )\n",
    "    run_start_ts = last_run_ts                      # Python datetime\n",
    "    data_cutoff_ts = last_run_ts                    # Python datetime\n",
    "    print(\"üîÅ Incremental run from:\", run_start_ts)\n",
    "else:\n",
    "    run_start_ts = datetime.fromisoformat(INFERENCE_START_TS)   # Python datetime\n",
    "    data_cutoff_ts = run_start_ts\n",
    "    print(\"üÜï First inference run from:\", INFERENCE_START_TS)\n",
    "\n",
    "refresh_cutoff_ts = F.current_timestamp() - F.expr(f\"INTERVAL {REFRESH_DAYS} DAYS\")\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ LOAD MODEL (INFERENCE ONLY)\n",
    "# ============================================================\n",
    "\n",
    "model = mlflow.pyfunc.load_model(MODEL_URI)\n",
    "print(\"‚úÖ ML model loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ LOAD & PREPARE BASE TABLES\n",
    "# ============================================================\n",
    "\n",
    "sales = spark.table(f\"{GOLD_DB}.gold_sales_enriched\")\n",
    "\n",
    "# ---- Customers with DERIVED AgeGroup ----\n",
    "customers = (\n",
    "    spark.table(f\"{SILVER_DB}.silver_customers\")\n",
    "         .select(\"CustomerID\", \"Age\", \"Location\", \"PreferredSeason\")\n",
    "         .withColumn(\n",
    "             \"AgeGroup\",\n",
    "             F.when(F.col(\"Age\").isNull(), \"Unknown\")\n",
    "              .when(F.col(\"Age\") < 18, \"Under 18\")\n",
    "              .when(F.col(\"Age\").between(18, 24), \"18-24\")\n",
    "              .when(F.col(\"Age\").between(25, 34), \"25-34\")\n",
    "              .when(F.col(\"Age\").between(35, 44), \"35-44\")\n",
    "              .when(F.col(\"Age\").between(45, 54), \"45-54\")\n",
    "              .when(F.col(\"Age\").between(55, 64), \"55-64\")\n",
    "              .otherwise(\"65+\")\n",
    "         )\n",
    "         .distinct()\n",
    ")\n",
    "\n",
    "products = spark.table(f\"{GOLD_DB}.gold_product_features\")\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ USER SEGMENTATION (STRICT POST‚ÄìJAN 5)\n",
    "# ============================================================\n",
    "\n",
    "new_active_users = (\n",
    "    sales.filter(F.col(\"EventTime\") > F.lit(run_start_ts))\n",
    "         .select(\"CustomerID\")\n",
    "         .distinct()\n",
    ")\n",
    "\n",
    "recent_active_users = (\n",
    "    sales.filter(F.col(\"EventTime\") >= refresh_cutoff_ts)\n",
    "         .filter(F.col(\"EventTime\") > F.lit(INFERENCE_START_TS))\n",
    "         .select(\"CustomerID\")\n",
    "         .distinct()\n",
    ")\n",
    "\n",
    "historical_active_users = sales.select(\"CustomerID\").distinct()\n",
    "\n",
    "cold_users = customers.join(\n",
    "    historical_active_users, \"CustomerID\", \"left_anti\"\n",
    ")\n",
    "\n",
    "ml_users = new_active_users.union(recent_active_users).distinct()\n",
    "\n",
    "print(\"New active users:\", new_active_users.count())\n",
    "print(\"Recently active users:\", recent_active_users.count())\n",
    "print(\"Cold users:\", cold_users.count())\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ ML + RULE BASED RECOMMENDATIONS (ACTIVE USERS)\n",
    "# ============================================================\n",
    "\n",
    "active_reco_pdf = None\n",
    "\n",
    "if ml_users.count() > 0:\n",
    "\n",
    "    fs_df = spark.table(f\"{GOLD_DB}.fs_canddiate_features\")\n",
    "    fs_active = fs_df.join(ml_users, \"CustomerID\", \"inner\")\n",
    "\n",
    "    if fs_active.count() > 0:\n",
    "\n",
    "        pdf = fs_active.toPandas().fillna(0)\n",
    "\n",
    "        FEATURE_COLS = [\n",
    "            \"src_same_category\",\n",
    "            \"src_brand_affinity\",\n",
    "            \"src_fbt\",\n",
    "            \"src_trending\",\n",
    "            \"src_age_group\",\n",
    "            \"src_location\",\n",
    "            \"user_views\",\n",
    "            \"user_carts\",\n",
    "            \"user_purchases\",\n",
    "            \"recent_7d_interaction\",\n",
    "            \"ProductRating\",\n",
    "            \"ReviewsCount\",\n",
    "            \"DiscountPercent\",\n",
    "            \"log_reviews\",\n",
    "            \"is_discounted\",\n",
    "            \"AvgReviewRating\",\n",
    "            \"age_group_encoded\",\n",
    "            \"num_sources\"\n",
    "        ]\n",
    "\n",
    "        pdf[\"prediction_score\"] = model.predict(pdf[FEATURE_COLS])\n",
    "\n",
    "        pdf[\"rule_score\"] = (\n",
    "            0.30 * pdf[\"src_trending\"] +\n",
    "            0.25 * pdf[\"is_discounted\"] +\n",
    "            0.20 * pdf[\"src_brand_affinity\"] +\n",
    "            0.10 * pdf[\"src_fbt\"] +\n",
    "            0.10 * pdf[\"src_same_category\"] +\n",
    "            0.05 * pdf[\"src_location\"] +\n",
    "            0.05 * pdf[\"src_age_group\"] +\n",
    "            0.05 * pdf[\"num_sources\"]\n",
    "        )\n",
    "\n",
    "        pdf[\"final_score\"] = 0.7 * pdf[\"prediction_score\"] + 0.3 * pdf[\"rule_score\"]\n",
    "\n",
    "        product_meta = products.select(\n",
    "            \"ProductID\", \"ProductName\", \"Brand\"\n",
    "        ).toPandas()\n",
    "\n",
    "        pdf = pdf.merge(product_meta, on=\"ProductID\", how=\"left\")\n",
    "\n",
    "        def build_reason(r):\n",
    "            reasons = []\n",
    "            if r.src_trending: reasons.append(\"Trending\")\n",
    "            if r.is_discounted: reasons.append(\"Discounted\")\n",
    "            if r.src_brand_affinity: reasons.append(\"Brand preference\")\n",
    "            if r.src_same_category: reasons.append(\"Same category\")\n",
    "            if r.src_fbt: reasons.append(\"Frequently bought together\")\n",
    "            if r.src_location: reasons.append(\"Popular in your location\")\n",
    "            if r.src_age_group: reasons.append(\"Popular in your age group\")\n",
    "            return \", \".join(reasons) if reasons else \"Personalized ranking\"\n",
    "\n",
    "        pdf[\"recommendation_reason\"] = pdf.apply(build_reason, axis=1)\n",
    "\n",
    "        topk = (\n",
    "            pdf.sort_values([\"CustomerID\", \"final_score\"], ascending=[True, False])\n",
    "               .groupby(\"CustomerID\")\n",
    "               .head(TOP_K)\n",
    "        )\n",
    "\n",
    "        active_reco_pdf = (\n",
    "            topk.groupby(\"CustomerID\")\n",
    "                .apply(lambda x: [\n",
    "                    {\n",
    "                        \"product_id\": int(r.ProductID),\n",
    "                        \"product_name\": r.ProductName,\n",
    "                        \"brand\": r.Brand,\n",
    "                        \"score\": round(r.final_score, 6),\n",
    "                        \"reason\": r.recommendation_reason\n",
    "                    }\n",
    "                    for r in x.itertuples()\n",
    "                ])\n",
    "                .reset_index(name=\"recommendations\")\n",
    "        )\n",
    "\n",
    "        active_reco_pdf[\"recommendation_type\"] = \"ML_HYBRID\"\n",
    "\n",
    "# ============================================================\n",
    "# 5Ô∏è‚É£ CONTEXT-AWARE COLD USER RECOMMENDATIONS\n",
    "# ============================================================\n",
    "\n",
    "cold_reco_pdf = None\n",
    "\n",
    "if cold_users.count() > 0:\n",
    "\n",
    "    cold_pdf = cold_users.toPandas()\n",
    "    product_pdf = products.select(\n",
    "        \"ProductID\", \"ProductName\", \"Brand\", \"ProductRating\", \"ReviewsCount\"\n",
    "    ).toPandas()\n",
    "\n",
    "    reco_rows = []\n",
    "\n",
    "    for _, u in cold_pdf.iterrows():\n",
    "\n",
    "        top_products = (\n",
    "            product_pdf\n",
    "            .sort_values([\"ProductRating\", \"ReviewsCount\"], ascending=False)\n",
    "            .head(TOP_K)\n",
    "        )\n",
    "\n",
    "        reco_rows.append({\n",
    "            \"CustomerID\": u[\"CustomerID\"],\n",
    "            \"recommendations\": [\n",
    "                {\n",
    "                    \"product_id\": int(r.ProductID),\n",
    "                    \"product_name\": r.ProductName,\n",
    "                    \"brand\": r.Brand,\n",
    "                    \"score\": 0.0,\n",
    "                    \"reason\": \"Popular products for new users\"\n",
    "                }\n",
    "                for r in top_products.itertuples()\n",
    "            ],\n",
    "            \"recommendation_type\": \"COLD_CONTEXTUAL\"\n",
    "        })\n",
    "\n",
    "    cold_reco_pdf = pd.DataFrame(reco_rows)\n",
    "\n",
    "# ============================================================\n",
    "# 6Ô∏è‚É£ UNION + SNAPSHOT WRITE (FIXED)\n",
    "# ============================================================\n",
    "\n",
    "frames = []\n",
    "\n",
    "if active_reco_pdf is not None:\n",
    "    frames.append(active_reco_pdf)\n",
    "\n",
    "if cold_reco_pdf is not None:\n",
    "    frames.append(cold_reco_pdf)\n",
    "\n",
    "if frames:\n",
    "    final_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    final_df[\"run_ts\"] = datetime.now()          # Python datetime\n",
    "    final_df[\"model_version\"] = MODEL_VERSION\n",
    "    final_df[\"data_cutoff_ts\"] = data_cutoff_ts # Python datetime ‚úÖ\n",
    "\n",
    "    spark.createDataFrame(final_df) \\\n",
    "         .write \\\n",
    "         .mode(\"append\") \\\n",
    "         .saveAsTable(RECO_TABLE)\n",
    "\n",
    "    print(\"‚úÖ Recommendations generated & stored successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No users to recommend in this run\")\n",
    "\n",
    "# ============================================================\n",
    "# 7Ô∏è‚É£ PREVIEW\n",
    "# ============================================================\n",
    "\n",
    "display(\n",
    "    spark.table(RECO_TABLE)\n",
    "         .orderBy(F.col(\"run_ts\").desc())\n",
    "         .limit(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e4c3a9-ff2b-49fd-8e96-21ccb6910154",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1767854551371}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"kusha_solutions.product_recomendation.user_recommendation_snapshots_live\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4510457317165481,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "9_inference_recommendation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
